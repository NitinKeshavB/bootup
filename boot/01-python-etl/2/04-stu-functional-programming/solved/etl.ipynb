{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# get alpaca api keys using this guide: https://alpaca.markets/docs/market-data/getting-started/#creating-an-alpaca-account-and-finding-your-api-keys\n",
    "from secrets_config import api_key_id, api_secret_key \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt \n",
    "\n",
    "def generate_datetime_ranges(\n",
    "        start_date:str=None, \n",
    "        end_date:str=None, \n",
    "    )->list:\n",
    "    \"\"\" \n",
    "    Generates a range of datetime ranges. \n",
    "    - start_date: provide a str with the format \"yyyy-mm-dd\"\n",
    "    - end_date: provide a str with the format \"yyyy-mm-dd\" \n",
    "    Usage example: \n",
    "    - generate_datetime_ranges(start_date=\"2020-01-01\", end_date=\"2022-01-02\")\n",
    "        returns: \n",
    "            [\n",
    "                'start_time': '2020-01-01T00:00:00.00Z', 'end_time': '2020-01-02T00:00:00.00Z'}, \n",
    "                {'start_time': '2020-01-02T00:00:00.00Z', 'end_time': '2020-01-03T00:00:00.00Z'}\n",
    "            ]\n",
    "    \"\"\"\n",
    "\n",
    "    date_range = []\n",
    "    if start_date is not None and end_date is not None: \n",
    "        dte_start_date = dt.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        dte_end_date = dt.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        date_range = [\n",
    "            {\n",
    "                \"start_time\": (dte_start_date + dt.timedelta(days=i)).strftime(\"%Y-%m-%dT%H:%M:%S.00Z\"),\n",
    "                \"end_time\": (dte_start_date + dt.timedelta(days=i) + dt.timedelta(days=1)).strftime(\"%Y-%m-%dT%H:%M:%S.00Z\"),\n",
    "            }\n",
    "        for i in range((dte_end_date - dte_start_date).days)]\n",
    "    else: \n",
    "        raise Exception(\"The parameters passed in results in no action being performed.\")\n",
    "\n",
    "    return date_range  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(\n",
    "        stock_ticker:str, \n",
    "        api_key_id:str, \n",
    "        api_secret_key:str, \n",
    "        start_date:str=None, \n",
    "        end_date:str=None\n",
    "    )->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract trades data from the Alpaca API. \n",
    "    - stock_ticker: ticker of a stock e.g. tsla \n",
    "    - api_key_id: api key id from Alpaca\n",
    "    - api_secret_key: api key secret from Alpaca\n",
    "    - start_date: date to begin extracting data from \n",
    "    - end_date: date to stop extracting data to \n",
    "    \n",
    "    Returns: \n",
    "    - DataFrame with the requested dates \n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = f\"https://data.alpaca.markets/v2/stocks/{stock_ticker}/trades\"\n",
    "    response_data = []\n",
    "    for date in generate_datetime_ranges(start_date=start_date, end_date=end_date):\n",
    "        start_time = date.get(\"start_time\")\n",
    "        end_time = date.get(\"end_time\")\n",
    "\n",
    "        params = {\n",
    "            \"start\": start_time,\n",
    "            \"end\": end_time\n",
    "        }\n",
    "\n",
    "        # auth example: https://alpaca.markets/docs/api-references/trading-api/\n",
    "        headers = {\n",
    "            \"APCA-API-KEY-ID\": api_key_id,\n",
    "            \"APCA-API-SECRET-KEY\": api_secret_key\n",
    "        }\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        if response.json().get(\"trades\") is not None: \n",
    "            response_data.extend(response.json().get(\"trades\"))\n",
    "    # read json data to a dataframe \n",
    "    df = pd.json_normalize(data=response_data, meta=[\"symbol\"])\n",
    "    return df \n",
    "\n",
    "# df = extract(\n",
    "#     stock_ticker=\"tsla\", \n",
    "#     api_key_id=api_key_id,\n",
    "#     api_secret_key=api_secret_key, \n",
    "#     start_date=\"2020-01-01\", \n",
    "#     days_from_start=5\n",
    "# )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exchange_codes(fp:str)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads exchange codes CSV file and returns a dataframe.\n",
    "    - fp: filepath to the exchange codes CSV file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fp)\n",
    "    return df\n",
    "# df_exchange_codes = extract_exchange_codes(\"data/exchange_codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(\n",
    "        df:pd.DataFrame, \n",
    "        df_exchange_codes:pd.DataFrame\n",
    "    )->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs transformation on dataframe produced from extract() function.\n",
    "    - df: dataframe produced from extract() function \n",
    "    - df_exchange_codes: dataframe produced from extract_exchange_codes() function \n",
    "\n",
    "    Returns: \n",
    "    - a transformed dataframe \n",
    "    \"\"\"\n",
    "    \n",
    "    df_quotes_renamed = df.rename(columns={\n",
    "        \"t\": \"timestamp\",\n",
    "        \"x\": \"exchange\",\n",
    "        \"p\": \"price\",\n",
    "        \"s\": \"size\",\n",
    "    })\n",
    "    df_quotes_selected = df_quotes_renamed[['timestamp', 'exchange', 'price', 'size']]\n",
    "    df_exchange = pd.merge(left=df_quotes_selected, right=df_exchange_codes, left_on=\"exchange\", right_on=\"exchange_code\").drop(columns=[\"exchange_code\", \"exchange\"]).rename(columns={\"exchange_name\": \"exchange\"})\n",
    "    # remove duplicates by doing a group by on the keys: timestamp and exchange\n",
    "    # get the mean of price, and sum of size\n",
    "    df_ask_bid_exchange_de_dup = df_exchange.groupby([\"timestamp\", \"exchange\"]).agg({\n",
    "        \"price\": \"mean\",\n",
    "        \"size\": \"sum\",\n",
    "    }).reset_index()\n",
    "    return df_ask_bid_exchange_de_dup\n",
    "\n",
    "# df_transform = transform(\n",
    "#     df=df,\n",
    "#     df_exchange_codes=df_exchange_codes\n",
    "# )\n",
    "# df_transform.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Table, Column, Integer, String, MetaData, Float # https://www.tutorialspoint.com/sqlalchemy/sqlalchemy_core_creating_table.htm\n",
    "from sqlalchemy.dialects import postgresql\n",
    "\n",
    "def load(\n",
    "        df: pd.DataFrame, \n",
    "        load_target:str, \n",
    "        load_method:str=\"upsert\",\n",
    "        target_file_directory:str=None,\n",
    "        target_file_name:str=None,\n",
    "        target_database_engine=None,\n",
    "        target_table_name:str=None\n",
    "    )->None:\n",
    "    \"\"\"\n",
    "    Load dataframe to either a file or a database. \n",
    "    - df: pandas dataframe to load.  \n",
    "    - load_target: choose either `file` or `database`.\n",
    "    - load_method: currently only `upsert` is supported. \n",
    "    - target_file_directory: directory where the file will be written to in parquet format.\n",
    "    - target_file_name: name of the target file e.g. stock.parquet. \n",
    "    - target_database_engine: SQLAlchemy engine for the target database. \n",
    "    - target_table_name: name of the SQL table to create and/or upsert data to. \n",
    "    \"\"\"\n",
    "    import os \n",
    "    if load_target == \"file\": \n",
    "        if load_method == \"upsert\": \n",
    "            # upsert (update and insert) data to a csv file \n",
    "            if target_file_name in os.listdir(f\"{target_file_directory}/\"): \n",
    "                df_current = pd.read_parquet(f\"{target_file_directory}/{target_file_name}\")\n",
    "                df_concat = pd.concat(objs=[df_current,df[~df.index.isin(df_current.index)]]) # ~: converts true to false, and false to true. \n",
    "                df_concat.to_parquet(f\"{target_file_directory}/{target_file_name}\")\n",
    "            else:\n",
    "                # create file \n",
    "                df.to_parquet(f\"{target_file_directory}/{target_file_name}\")\n",
    "    elif load_target == \"database\": \n",
    "        # create target table if not exists \n",
    "        meta = MetaData()\n",
    "        stock_price_tesla_table = Table(\n",
    "            target_table_name, meta, \n",
    "            Column(\"timestamp\", String, primary_key=True),\n",
    "            Column(\"exchange\", String, primary_key=True),\n",
    "            Column(\"price\", Float),\n",
    "            Column(\"size\", Integer)\n",
    "        )\n",
    "        meta.create_all(target_database_engine) # creates table if it does not exist \n",
    "        insert_statement = postgresql.insert(stock_price_tesla_table).values(df.to_dict(orient='records'))\n",
    "        upsert_statement = insert_statement.on_conflict_do_update(\n",
    "            index_elements=['timestamp', 'exchange'],\n",
    "            set_={c.key: c for c in insert_statement.excluded if c.key not in ['timestamp','exchange']})\n",
    "        target_database_engine.execute(upsert_statement)\n",
    "\n",
    "# from sqlalchemy import create_engine, Table, Column, Integer, String, MetaData, Float # https://www.tutorialspoint.com/sqlalchemy/sqlalchemy_core_creating_table.htm\n",
    "# from sqlalchemy.engine import URL\n",
    "# from sqlalchemy.dialects import postgresql\n",
    "# from secrets_config import db_user, db_password, db_server_name, db_database_name\n",
    "# from sqlalchemy.schema import CreateTable \n",
    "\n",
    "# # create connection to database \n",
    "# connection_url = URL.create(\n",
    "#     drivername = \"postgresql+pg8000\", \n",
    "#     username = db_user,\n",
    "#     password = db_password,\n",
    "#     host = db_server_name, \n",
    "#     port = 5432,\n",
    "#     database = db_database_name, \n",
    "# )\n",
    "\n",
    "# engine = create_engine(connection_url)\n",
    "\n",
    "# load(\n",
    "#     df=df_transform,\n",
    "#     load_target=\"database\",\n",
    "#     target_database_engine=engine,\n",
    "#     target_table_name=\"new_tesla_table\"\n",
    "# )\n",
    "\n",
    "# load(\n",
    "#     df=df_transform,\n",
    "#     load_target=\"file\",\n",
    "#     target_file_directory=\"data\",\n",
    "#     target_file_name=\"tesla.parquet\",\n",
    "# )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline()->bool:\n",
    "    # extract data \n",
    "    df = extract(\n",
    "        stock_ticker=\"tsla\", \n",
    "        api_key_id=api_key_id,\n",
    "        api_secret_key=api_secret_key, \n",
    "        start_date=\"2020-01-01\", \n",
    "        end_date=\"2020-01-05\"\n",
    "    )   \n",
    "    df_exchange_codes = extract_exchange_codes(\"data/exchange_codes.csv\")\n",
    "\n",
    "    # transform data\n",
    "    df_transform = transform(\n",
    "        df=df,\n",
    "        df_exchange_codes=df_exchange_codes\n",
    "    )\n",
    "\n",
    "    # load file (upsert)\n",
    "    load(\n",
    "        df=df_transform,\n",
    "        load_target=\"file\",\n",
    "        target_file_directory=\"data\",\n",
    "        target_file_name=\"tesla.parquet\",\n",
    "    )   \n",
    "    from sqlalchemy import create_engine, Table, Column, Integer, String, MetaData, Float # https://www.tutorialspoint.com/sqlalchemy/sqlalchemy_core_creating_table.htm\n",
    "    from sqlalchemy.engine import URL\n",
    "    from sqlalchemy.dialects import postgresql\n",
    "    from secrets_config import db_user, db_password, db_server_name, db_database_name\n",
    "    from sqlalchemy.schema import CreateTable \n",
    "\n",
    "    # create connection to database \n",
    "    connection_url = URL.create(\n",
    "        drivername = \"postgresql+pg8000\", \n",
    "        username = db_user,\n",
    "        password = db_password,\n",
    "        host = db_server_name, \n",
    "        port = 5432,\n",
    "        database = db_database_name, \n",
    "    )\n",
    "\n",
    "    engine = create_engine(connection_url)\n",
    "\n",
    "    # load database (upsert)\n",
    "    load(\n",
    "        df=df_transform,\n",
    "        load_target=\"database\",\n",
    "        target_database_engine=engine,\n",
    "        target_table_name=\"tesla_stock\"\n",
    "    )   \n",
    "    return True\n",
    "\n",
    "# run the pipeline\n",
    "pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec7476cd5298a73f69e8eecc398cdeac6e308767034e2d84faebe029453106ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
