{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from secrets_config import api_key # https://home.openweathermap.org/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_city(\n",
    "        api_key:str,\n",
    "        city_name:str=None,\n",
    "        temperature_units:str=\"metric\"\n",
    "    )->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracting data from the weather API. \n",
    "    - api_key: api key \n",
    "    - city name: name of the city e.g. perth\n",
    "    - temperature_units: choose one of \"metric\" or \"imperial\" or \"standard\"\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"q\": city_name,\n",
    "        \"units\": temperature_units,\n",
    "        \"appid\": api_key\n",
    "    }\n",
    "    response = requests.get(f\"http://api.openweathermap.org/data/2.5/weather\", params=params)\n",
    "    if response.status_code == 200: \n",
    "        weather_data = response.json()\n",
    "    else: \n",
    "        raise Exception(\"Extracting weather api data failed. Please check if API limits have been reached.\")\n",
    "    df_weather_cities = pd.json_normalize(weather_data)\n",
    "    return df_weather_cities\n",
    "\n",
    "# df = extract_city(\n",
    "#     api_key=api_key, \n",
    "#     city_name=\"perth\",\n",
    "#     temperature_units=\"metric\"\n",
    "# )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(\n",
    "        api_key:str,\n",
    "        fp_cities:str,\n",
    "        temperature_units:str=\"metric\"\n",
    "    )->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform extraction using a filepath which contains a list of cities. \n",
    "    - api_key: api key \n",
    "    - fp_cities: filepath to a CSV file containing a list of cities \n",
    "    - temperature_units: choose one of \"metric\" or \"imperial\" or \"standard\"\n",
    "    \"\"\"\n",
    "\n",
    "    # read list of cities\n",
    "    df_cities = pd.read_csv(fp_cities)\n",
    "    # request data for each city (json) and push to a list \n",
    "    df_concat = pd.DataFrame()\n",
    "    for city_name in df_cities[\"city_name\"]:\n",
    "        df_extracted = extract_city(api_key=api_key,city_name=city_name,temperature_units=temperature_units)\n",
    "        df_concat = pd.concat([df_concat,df_extracted])\n",
    "    return df_concat.reset_index().drop(labels=[\"index\"], axis=1)\n",
    "\n",
    "# df = extract(\n",
    "#     api_key=api_key, \n",
    "#     fp_cities=\"data/australian_capital_cities.csv\",\n",
    "#     temperature_units=\"metric\"\n",
    "# )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_population(\n",
    "        fp_population:str\n",
    "    )->pd.DataFrame:\n",
    "    \"\"\"Extracts data from the population file\"\"\"\n",
    "    df_population = pd.read_csv(fp_population)\n",
    "    return df_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(\n",
    "        df:pd.DataFrame, \n",
    "        df_population:str=None\n",
    "    )->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform the raw dataframes. \n",
    "    - df: the dataframe produced from extract(). \n",
    "    - df_population: the dataframe produced from extract_population(). \n",
    "    \"\"\"\n",
    "    # set city names to lowercase \n",
    "    df[\"city_name\"] = df[\"name\"].str.lower()\n",
    "    df_merged = pd.merge(left=df, right=df_population, on=[\"city_name\"])\n",
    "    df_selected = df_merged[[\"dt\", \"id\", \"name\", \"main.temp\", \"population\"]] \n",
    "    df_selected[\"unique_id\"] = df_selected[\"dt\"].astype(str) + df_selected[\"id\"].astype(str)\n",
    "    # convert unix timestamp column to datetime \n",
    "    df_selected[\"dt\"] = pd.to_datetime(df_selected[\"dt\"], unit=\"s\")\n",
    "    # rename colum names to more meaningful names\n",
    "    df_selected = df_selected.rename(columns={\n",
    "        \"dt\": \"datetime\",\n",
    "        \"main.temp\": \"temperature\"\n",
    "    })\n",
    "    df_selected = df_selected.set_index([\"unique_id\"])\n",
    "    return df_selected \n",
    "\n",
    "# df_transformed = transform(df=df, fp_population=\"data/australian_city_population.csv\")\n",
    "# df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def load(\n",
    "    df:pd.DataFrame,\n",
    "    load_target:str, \n",
    "    load_method:str=\"overwrite\",\n",
    "    target_file_directory:str=None,\n",
    "    target_file_name:str=None,\n",
    "    target_database_engine=None,\n",
    "    target_table_name:str=None\n",
    "    )->None:\n",
    "    \"\"\"\n",
    "    Load dataframe to either a file or a database. \n",
    "    - df: pandas dataframe to load.  \n",
    "    - load_target: choose either `file` or `database`.\n",
    "    - load_method: choose either `overwrite` or `upsert`. defaults to `overwrite`. \n",
    "    - target_file_directory: directory where the file will be written to in parquet format.\n",
    "    - target_file_name: name of the target file e.g. stock.parquet. \n",
    "    - target_database_engine: SQLAlchemy engine for the target database. \n",
    "    - target_table_name: name of the SQL table to create and/or upsert data to. \n",
    "    \"\"\"\n",
    "    if load_target.lower() == \"file\": \n",
    "        if load_method.lower() == \"overwrite\": \n",
    "            df.to_parquet(f\"{target_file_directory}/{target_file_name}\")\n",
    "        elif load_target.lower() == \"upsert\": \n",
    "            if target_file_name in os.listdir(f\"{target_file_directory}/\"): \n",
    "                df_current = pd.read_parquet(f\"{target_file_directory}/{target_file_name}\")\n",
    "                df_concat = pd.concat(objs=[df_current,df[~df.index.isin(df_current.index)]]) # ~: converts true to false, and false to true. \n",
    "                df_concat.to_parquet(f\"{target_file_directory}/{target_file_name}\")\n",
    "            else:\n",
    "                df.to_parquet(f\"{target_file_directory}/{target_file_name}\")\n",
    "\n",
    "    elif load_target.lower() == \"database\": \n",
    "        from sqlalchemy import Table, Column, Integer, String, MetaData, Float\n",
    "        from sqlalchemy.dialects import postgresql\n",
    "        if load_method.lower() == \"overwrite\": \n",
    "            df.to_sql(target_table_name, target_database_engine)\n",
    "        elif load_method.lower() == \"upsert\":\n",
    "            meta = MetaData()\n",
    "            weather_table = Table(\n",
    "                target_table_name, meta, \n",
    "                Column(\"datetime\", String, primary_key=True),\n",
    "                Column(\"id\", Integer, primary_key=True),\n",
    "                Column(\"name\", String),\n",
    "                Column(\"temperature\", Float),\n",
    "                Column(\"population\", Integer)\n",
    "            )\n",
    "            meta.create_all(target_database_engine) # creates table if it does not exist \n",
    "            insert_statement = postgresql.insert(weather_table).values(df.to_dict(orient='records'))\n",
    "            upsert_statement = insert_statement.on_conflict_do_update(\n",
    "                index_elements=['id', 'datetime'],\n",
    "                set_={c.key: c for c in insert_statement.excluded if c.key not in ['id', 'datetime']})\n",
    "            target_database_engine.execute(upsert_statement)\n",
    "\n",
    "\n",
    "# load(\n",
    "#     df=df_transformed,\n",
    "#     load_target=\"file\", \n",
    "#     load_method=\"upsert\", \n",
    "#     target_file_directory=\"data\", \n",
    "#     target_file_name=\"weather.parquet\", \n",
    "#     existing_file_directory=\"data\",\n",
    "#     existing_file_name=\"weather.parquet\"\n",
    "# )\n",
    "\n",
    "# load(\n",
    "#     df=df_transformed,\n",
    "#     load_target=\"database\", \n",
    "#     load_method=\"upsert\", \n",
    "#     target_table_name=\"weather_upsert\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/62h2k_3s07vbf145yklkj8740000gn/T/ipykernel_45231/3173341717.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[\"unique_id\"] = df_selected[\"dt\"].astype(str) + df_selected[\"id\"].astype(str)\n",
      "/var/folders/36/62h2k_3s07vbf145yklkj8740000gn/T/ipykernel_45231/3173341717.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[\"dt\"] = pd.to_datetime(df_selected[\"dt\"], unit=\"s\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pipeline()->bool:\n",
    "    \"\"\"\n",
    "    Pipeline performs ETL from the Weather Data API and outputs transformed data to a parquet file and to a postgres database. \n",
    "    \"\"\"\n",
    "    # extract \n",
    "    df = extract(\n",
    "        api_key=api_key, \n",
    "        fp_cities=\"data/australian_capital_cities.csv\",\n",
    "        temperature_units=\"metric\"\n",
    "    )\n",
    "    df_population = extract_population(fp_population=\"data/australian_city_population.csv\")\n",
    "\n",
    "    # transform \n",
    "    df_transformed = transform(df=df, df_population=df_population)\n",
    "\n",
    "    # load file \n",
    "    load(\n",
    "        df=df_transformed,\n",
    "        load_target=\"file\", \n",
    "        load_method=\"upsert\", \n",
    "        target_file_directory=\"data\", \n",
    "        target_file_name=\"weather.parquet\", \n",
    "    )\n",
    "\n",
    "    from sqlalchemy import create_engine, Table, Column, Integer, String, MetaData, Float # https://www.tutorialspoint.com/sqlalchemy/sqlalchemy_core_creating_table.htm\n",
    "    from sqlalchemy.engine import URL\n",
    "    from sqlalchemy.dialects import postgresql\n",
    "    from secrets_config import db_user, db_password, db_server_name, db_database_name\n",
    "    from sqlalchemy.schema import CreateTable \n",
    "\n",
    "    # create connection to database \n",
    "    connection_url = URL.create(\n",
    "        drivername = \"postgresql+pg8000\", \n",
    "        username = db_user,\n",
    "        password = db_password,\n",
    "        host = db_server_name, \n",
    "        port = 5432,\n",
    "        database = db_database_name, \n",
    "    )\n",
    "    engine = create_engine(connection_url)\n",
    "\n",
    "    # load database \n",
    "    load(\n",
    "        df=df_transformed,\n",
    "        load_target=\"database\", \n",
    "        load_method=\"upsert\", \n",
    "        target_database_engine=engine,\n",
    "        target_table_name=\"weather\"\n",
    "    )\n",
    "\n",
    "    return True \n",
    "    \n",
    "pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec7476cd5298a73f69e8eecc398cdeac6e308767034e2d84faebe029453106ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
