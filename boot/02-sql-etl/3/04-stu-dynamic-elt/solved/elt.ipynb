{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from secrets_config import source_db_user, source_db_password, source_db_server_name, source_db_database_name\n",
    "import jinja2 as j2 \n",
    "\n",
    "# import libraries for sql \n",
    "from sqlalchemy import create_engine, Table, Column, Integer, String, MetaData, Float, JSON \n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy.dialects import postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection to database \n",
    "source_connection_url = URL.create(\n",
    "    drivername = \"postgresql+pg8000\", \n",
    "    username = source_db_user,\n",
    "    password = source_db_password,\n",
    "    host = source_db_server_name, \n",
    "    port = 5432,\n",
    "    database = source_db_database_name, \n",
    ")\n",
    "\n",
    "source_engine = create_engine(source_connection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging \n",
    "import datetime as dt \n",
    "import numpy as np\n",
    "\n",
    "def get_incremental_value(table_name, path=\"extract_log\"):\n",
    "    df = pd.read_csv(f\"{path}/{table_name}.csv\")\n",
    "    return df[df[\"log_date\"] == df[\"log_date\"].max()][\"incremental_value\"].values[0]\n",
    "\n",
    "def is_incremental(table:str, path:str)->bool:\n",
    "    # read sql contents into a variable \n",
    "    with open(f\"{path}/{table}.sql\") as f: \n",
    "        raw_sql = f.read()\n",
    "    try: \n",
    "        config = j2.Template(raw_sql).make_module().config \n",
    "        return config[\"extract_type\"].lower() == \"incremental\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def upsert_incremental_log(log_path, table_name, incremental_value)->bool:\n",
    "    if f\"{table_name}.csv\" in os.listdir(log_path):\n",
    "        df_existing_incremental_log = pd.read_csv(f\"{log_path}/{table_name}.csv\")\n",
    "        df_incremental_log = pd.DataFrame(data={\n",
    "            \"log_date\": [dt.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")], \n",
    "            \"incremental_value\": [incremental_value]\n",
    "        })\n",
    "        df_updated_incremental_log = pd.concat([df_existing_incremental_log,df_incremental_log])\n",
    "        df_updated_incremental_log.to_csv(f\"{log_path}/{table_name}.csv\", index=False)\n",
    "    else: \n",
    "        df_incremental_log = pd.DataFrame(data={\n",
    "            \"log_date\": [dt.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")], \n",
    "            \"incremental_value\": [incremental_value]\n",
    "        })\n",
    "        df_incremental_log.to_csv(f\"{log_path}/{table_name}.csv\", index=False)\n",
    "    return True \n",
    "\n",
    "def extract_from_database(table_name, engine, path=\"extract_queries\")->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds models with a matching file name in the models_path folder. \n",
    "    - `table_name`: the name of the table (without .sql)\n",
    "    - `path`: the path to the extract queries directory containing the sql files. defaults to `extract_queries`\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s][%(asctime)s]: %(message)s\")\n",
    "    \n",
    "    logging.info(f\"Extracting table: {table_name}\")\n",
    "    if f\"{table_name}.sql\" in os.listdir(path):\n",
    "        # read sql contents into a variable \n",
    "        with open(f\"{path}/{table_name}.sql\") as f: \n",
    "            raw_sql = f.read()\n",
    "        # get config \n",
    "        config = j2.Template(raw_sql).make_module().config \n",
    "        \n",
    "        if is_incremental(table=table_name, path=path): \n",
    "            incremental_path = \"extract_log\"\n",
    "            if not os.path.exists(incremental_path): \n",
    "                os.mkdir(incremental_path)\n",
    "            if f\"{table_name}.csv\" in os.listdir(incremental_path):\n",
    "                # get incremental value and perform incremental extract \n",
    "                current_max_incremental_value = get_incremental_value(table_name, path=incremental_path)\n",
    "                parsed_sql = j2.Template(raw_sql).render(source_table = table_name, engine=engine, is_incremental=True, incremental_value=current_max_incremental_value)\n",
    "                # execute incremental extract\n",
    "                df = pd.read_sql(sql=parsed_sql, con=engine)\n",
    "                # update max incremental value \n",
    "                if len(df) > 0: \n",
    "                    max_incremental_value = df[config[\"incremental_column\"]].max()\n",
    "                else: \n",
    "                    max_incremental_value = current_max_incremental_value\n",
    "                upsert_incremental_log(log_path=incremental_path, table_name=table_name, incremental_value=max_incremental_value)\n",
    "                logging.info(f\"Successfully extracted table: {table_name}, rows extracted: {len(df)}\")\n",
    "                return df \n",
    "            else: \n",
    "                # parse sql using jinja \n",
    "                parsed_sql = j2.Template(raw_sql).render(source_table = table_name, engine=engine)\n",
    "                # perform full extract \n",
    "                df = pd.read_sql(sql=parsed_sql, con=engine)\n",
    "                # store latest incremental value \n",
    "                max_incremental_value = df[config[\"incremental_column\"]].max()\n",
    "                upsert_incremental_log(log_path=incremental_path, table_name=table_name, incremental_value=max_incremental_value)\n",
    "                logging.info(f\"Successfully extracted table: {table_name}, rows extracted: {len(df)}\")\n",
    "                return df \n",
    "        else: \n",
    "            # parse sql using jinja \n",
    "            parsed_sql = j2.Template(raw_sql).render(source_table = table_name, engine=engine)\n",
    "            # perform full extract \n",
    "            df = pd.read_sql(sql=parsed_sql, con=engine)\n",
    "            logging.info(f\"Successfully extracted table: {table_name}, rows extracted: {len(df)}\")\n",
    "            return df \n",
    "    else: \n",
    "        logging.error(f\"Could not find table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_columns(table:str, path:str=\"extract_queries\")->list: \n",
    "    \"\"\"\n",
    "    get a list of key columns from the .sql file. key_columns have to be expressed as `{% set key_columns = [\"keyA\", \"keyB\"]%}` in the .sql file. \n",
    "    - `table`: name of the sql file without .sql \n",
    "    - `path`: path to the sql file \n",
    "    \"\"\"\n",
    "    # read sql contents into a variable \n",
    "    with open(f\"{path}/{table}.sql\") as f: \n",
    "        raw_sql = f.read()\n",
    "    try: \n",
    "        key_columns = j2.Template(raw_sql).make_module().config[\"key_columns\"] # get key columns \n",
    "        return key_columns\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Integer, String, Float, JSON , DateTime, Boolean, BigInteger, Numeric\n",
    "\n",
    "\n",
    "def get_sqlalchemy_column(column_name:str , source_datatype:str, primary_key:bool=False)->Column:\n",
    "    \"\"\"\n",
    "    A helper function that returns a SQLAlchemy column by mapping a pandas dataframe datatypes to sqlalchemy datatypes \n",
    "    \"\"\"\n",
    "    dtype_map = {\n",
    "        \"int64\": BigInteger, \n",
    "        \"object\": String, \n",
    "        \"datetime64[ns]\": DateTime, \n",
    "        \"float64\": Numeric,\n",
    "        \"bool\": Boolean\n",
    "    }\n",
    "    column = Column(column_name, dtype_map[source_datatype], primary_key=primary_key) \n",
    "    return column\n",
    "\n",
    "def generate_sqlalchemy_schema(df: pd.DataFrame, key_columns:list, table_name, meta): \n",
    "    \"\"\"\n",
    "    Generates a sqlalchemy table schema that shall be used to create the target table and perform insert/upserts. \n",
    "    \"\"\"\n",
    "    schema = []\n",
    "    for column in [{\"column_name\": col[0], \"source_datatype\": col[1]} for col in zip(df.columns, [dtype.name for dtype in df.dtypes])]:\n",
    "        schema.append(get_sqlalchemy_column(**column, primary_key=column[\"column_name\"] in key_columns))\n",
    "    return Table(table_name, meta, *schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def upsert_in_chunks(df:pd.DataFrame, engine, table_schema:Table, key_columns:list, chunksize:int=1000)->bool:\n",
    "    \"\"\"\n",
    "    performs the upsert with several rows at a time (i.e. a chunk of rows). this is better suited for very large sql statements that need to be broken into several steps. \n",
    "    \"\"\"\n",
    "    max_length = len(df)\n",
    "    df = df.replace({np.nan: None})\n",
    "    for i in range(0, max_length, chunksize):\n",
    "        if i + chunksize >= max_length: \n",
    "            lower_bound = i\n",
    "            upper_bound = max_length \n",
    "        else: \n",
    "            lower_bound = i \n",
    "            upper_bound = i + chunksize\n",
    "        insert_statement = postgresql.insert(table_schema).values(df.iloc[lower_bound:upper_bound].to_dict(orient='records'))\n",
    "        upsert_statement = insert_statement.on_conflict_do_update(\n",
    "            index_elements=key_columns,\n",
    "            set_={c.key: c for c in insert_statement.excluded if c.key not in key_columns})\n",
    "        logging.info(f\"Inserting chunk: [{lower_bound}:{upper_bound}] out of index {max_length}\")\n",
    "        result = engine.execute(upsert_statement)\n",
    "    return True \n",
    "\n",
    "def upsert_all(df:pd.DataFrame, engine, table_schema:Table, key_columns:list)->bool:\n",
    "    \"\"\"\n",
    "    performs the upsert with all rows at once. this may cause timeout issues if the sql statement is very large. \n",
    "    \"\"\"\n",
    "    insert_statement = postgresql.insert(table_schema).values(df.to_dict(orient='records'))\n",
    "    upsert_statement = insert_statement.on_conflict_do_update(\n",
    "        index_elements=key_columns,\n",
    "        set_={c.key: c for c in insert_statement.excluded if c.key not in key_columns})\n",
    "    result = engine.execute(upsert_statement)\n",
    "    logging.info(f\"Insert/updated rows: {result.rowcount}\")\n",
    "    return True \n",
    "\n",
    "def upsert_to_database(df: pd.DataFrame, table_name: str, key_columns: str, engine, chunksize:int=1000)->bool: \n",
    "    \"\"\"\n",
    "    Upsert dataframe to a database table \n",
    "    - `df`: pandas dataframe \n",
    "    - `table`: name of the target table \n",
    "    - `key_columns`: name of key columns to be used for upserting \n",
    "    - `engine`: connection engine to database \n",
    "    - `chunksize`: if chunksize greater than 0 is specified, then the rows will be inserted in the specified chunksize. e.g. 1000 rows at a time. \n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s][%(asctime)s]: %(message)s\")\n",
    "    meta = MetaData()\n",
    "    logging.info(f\"Generating table schema: {table_name}\")\n",
    "    table_schema = generate_sqlalchemy_schema(df=df, key_columns=key_columns,table_name=table_name, meta=meta)\n",
    "    meta.create_all(engine)\n",
    "    logging.info(f\"Table schema generated: {table_name}\")\n",
    "    logging.info(f\"Writing to table: {table_name}\")\n",
    "    if chunksize > 0:\n",
    "        upsert_in_chunks(df=df, engine=engine, table_schema=table_schema, key_columns=key_columns, chunksize=chunksize)\n",
    "    else: \n",
    "        upsert_all(df=df, engine=engine, table_schema=table_schema, key_columns=key_columns)\n",
    "    logging.info(f\"Successful write to table: {table_name}\")\n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_to_database(df: pd.DataFrame, table_name: str, engine)->bool: \n",
    "    \"\"\"\n",
    "    Upsert dataframe to a database table \n",
    "    - `df`: pandas dataframe \n",
    "    - `table`: name of the target table \n",
    "    - `engine`: connection engine to database \n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s][%(asctime)s]: %(message)s\")\n",
    "    logging.info(f\"Writing to table: {table_name}\")\n",
    "    df.to_sql(name=table_name, con=engine, if_exists=\"replace\", index=False)\n",
    "    logging.info(f\"Successful write to table: {table_name}, rows inserted/updated: {len(df)}\")\n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrets_config import target_db_user, target_db_password, target_db_server_name, target_db_database_name\n",
    "# create connection to database \n",
    "target_connection_url = URL.create(\n",
    "    drivername = \"postgresql+pg8000\", \n",
    "    username = target_db_user,\n",
    "    password = target_db_password,\n",
    "    host = target_db_server_name, \n",
    "    port = 5432,\n",
    "    database = target_db_database_name, \n",
    ")\n",
    "\n",
    "target_engine = create_engine(target_connection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_load_pipeline(source_engine, target_engine, path=\"extract_queries\"): \n",
    "    for file in os.listdir(path):\n",
    "        table_name = file.replace(\".sql\", \"\")\n",
    "        df = extract_from_database(table_name=table_name, engine=source_engine, path=path)\n",
    "        if is_incremental(table=table_name, path=path):\n",
    "            key_columns = get_key_columns(table=table_name, path=path)\n",
    "            upsert_to_database(df=df, table_name=table_name, key_columns=key_columns, engine=target_engine, chunksize=1000)\n",
    "        else: \n",
    "            overwrite_to_database(df=df, table_name=table_name, engine=target_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022-07-24 18:04:37,277]: Extracting table: customer\n",
      "[INFO][2022-07-24 18:04:37,362]: Successfully extracted table: customer, rows extracted: 599\n",
      "[INFO][2022-07-24 18:04:37,365]: Writing to table: customer\n",
      "[INFO][2022-07-24 18:04:37,789]: Successful write to table: customer, rows inserted/updated: 599\n",
      "[INFO][2022-07-24 18:04:37,789]: Extracting table: film_category\n",
      "[INFO][2022-07-24 18:04:37,828]: Successfully extracted table: film_category, rows extracted: 1000\n",
      "[INFO][2022-07-24 18:04:37,830]: Writing to table: film_category\n",
      "[INFO][2022-07-24 18:04:38,269]: Successful write to table: film_category, rows inserted/updated: 1000\n",
      "[INFO][2022-07-24 18:04:38,270]: Extracting table: film\n",
      "[INFO][2022-07-24 18:04:38,327]: Successfully extracted table: film, rows extracted: 1000\n",
      "[INFO][2022-07-24 18:04:38,329]: Writing to table: film\n",
      "[INFO][2022-07-24 18:04:38,981]: Successful write to table: film, rows inserted/updated: 1000\n",
      "[INFO][2022-07-24 18:04:38,982]: Extracting table: staff\n",
      "[INFO][2022-07-24 18:04:38,989]: Successfully extracted table: staff, rows extracted: 2\n",
      "[INFO][2022-07-24 18:04:38,991]: Writing to table: staff\n",
      "[INFO][2022-07-24 18:04:39,034]: Successful write to table: staff, rows inserted/updated: 2\n",
      "[INFO][2022-07-24 18:04:39,035]: Extracting table: rental\n",
      "[INFO][2022-07-24 18:04:39,068]: Successfully extracted table: rental, rows extracted: 0\n",
      "[INFO][2022-07-24 18:04:39,072]: Generating table schema: rental\n",
      "[INFO][2022-07-24 18:04:39,076]: Table schema generated: rental\n",
      "[INFO][2022-07-24 18:04:39,076]: Writing to table: rental\n",
      "[INFO][2022-07-24 18:04:39,077]: Successful write to table: rental\n",
      "[INFO][2022-07-24 18:04:39,077]: Extracting table: category\n",
      "[INFO][2022-07-24 18:04:39,087]: Successfully extracted table: category, rows extracted: 16\n",
      "[INFO][2022-07-24 18:04:39,089]: Writing to table: category\n",
      "[INFO][2022-07-24 18:04:39,137]: Successful write to table: category, rows inserted/updated: 16\n",
      "[INFO][2022-07-24 18:04:39,137]: Extracting table: inventory\n",
      "[INFO][2022-07-24 18:04:39,166]: Successfully extracted table: inventory, rows extracted: 0\n",
      "[INFO][2022-07-24 18:04:39,170]: Generating table schema: inventory\n",
      "[INFO][2022-07-24 18:04:39,174]: Table schema generated: inventory\n",
      "[INFO][2022-07-24 18:04:39,174]: Writing to table: inventory\n",
      "[INFO][2022-07-24 18:04:39,175]: Successful write to table: inventory\n",
      "[INFO][2022-07-24 18:04:39,176]: Extracting table: address\n",
      "[INFO][2022-07-24 18:04:39,212]: Successfully extracted table: address, rows extracted: 603\n",
      "[INFO][2022-07-24 18:04:39,214]: Writing to table: address\n",
      "[INFO][2022-07-24 18:04:39,538]: Successful write to table: address, rows inserted/updated: 603\n",
      "[INFO][2022-07-24 18:04:39,538]: Extracting table: city\n",
      "[INFO][2022-07-24 18:04:39,574]: Successfully extracted table: city, rows extracted: 600\n",
      "[INFO][2022-07-24 18:04:39,575]: Writing to table: city\n",
      "[INFO][2022-07-24 18:04:39,861]: Successful write to table: city, rows inserted/updated: 600\n",
      "[INFO][2022-07-24 18:04:39,862]: Extracting table: country\n",
      "[INFO][2022-07-24 18:04:39,873]: Successfully extracted table: country, rows extracted: 109\n",
      "[INFO][2022-07-24 18:04:39,875]: Writing to table: country\n",
      "[INFO][2022-07-24 18:04:39,960]: Successful write to table: country, rows inserted/updated: 109\n",
      "[INFO][2022-07-24 18:04:39,961]: Extracting table: payment\n",
      "[INFO][2022-07-24 18:04:39,976]: Successfully extracted table: payment, rows extracted: 0\n",
      "[INFO][2022-07-24 18:04:39,981]: Generating table schema: payment\n",
      "[INFO][2022-07-24 18:04:39,984]: Table schema generated: payment\n",
      "[INFO][2022-07-24 18:04:39,985]: Writing to table: payment\n",
      "[INFO][2022-07-24 18:04:39,985]: Successful write to table: payment\n"
     ]
    }
   ],
   "source": [
    "extract_load_pipeline(\n",
    "    source_engine=source_engine, \n",
    "    target_engine=target_engine, \n",
    "    path=\"extract_queries\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging \n",
    "\n",
    "def build_model(model, engine, models_path=\"models\")->bool:\n",
    "    \"\"\"\n",
    "    Builds models with a matching file name in the models_path folder. \n",
    "    - `model`: the name of the model (without .sql)\n",
    "    - `models_path`: the path to the models directory containing the sql files. defaults to `models`\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s][%(asctime)s]: %(message)s\")\n",
    "    \n",
    "    if f\"{model}.sql\" in os.listdir(models_path):\n",
    "        logging.info(f\"Building model: {model}\")\n",
    "    \n",
    "        # read sql contents into a variable \n",
    "        with open(f\"{models_path}/{model}.sql\") as f: \n",
    "            raw_sql = f.read()\n",
    "\n",
    "        # parse sql using jinja \n",
    "        parsed_sql = j2.Template(raw_sql).render(target_table = model, engine=engine)\n",
    "\n",
    "        # execute parsed sql \n",
    "        result = engine.execute(parsed_sql)\n",
    "        logging.info(f\"Successfully built model: {model}, rows inserted/updated: {result.rowcount}\")\n",
    "        return True \n",
    "    else: \n",
    "        logging.error(f\"Could not find model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TopologicalSorter\n",
    "from graphlib import TopologicalSorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('staging_films', 'serving_sales_customer', 'serving_sales_cumulative', 'serving_sales_film', 'serving_films_popular')\n"
     ]
    }
   ],
   "source": [
    "# create a DAG of models using TopologicalSorter\n",
    "ts = TopologicalSorter()\n",
    "ts.add(\"staging_films\")\n",
    "ts.add(\"serving_sales_film\", \"staging_films\")\n",
    "ts.add(\"serving_films_popular\", \"staging_films\")\n",
    "ts.add(\"serving_sales_customer\")\n",
    "ts.add(\"serving_sales_cumulative\")\n",
    "dag = tuple(ts.static_order())\n",
    "print(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2022-07-24 18:04:40,487]: Building model: staging_films\n",
      "[INFO][2022-07-24 18:04:40,568]: Successfully built model: staging_films, rows inserted/updated: 958\n",
      "[INFO][2022-07-24 18:04:40,569]: Building model: serving_sales_customer\n",
      "[INFO][2022-07-24 18:04:40,618]: Successfully built model: serving_sales_customer, rows inserted/updated: 599\n",
      "[INFO][2022-07-24 18:04:40,619]: Building model: serving_sales_cumulative\n",
      "[INFO][2022-07-24 18:04:40,687]: Successfully built model: serving_sales_cumulative, rows inserted/updated: 14596\n",
      "[INFO][2022-07-24 18:04:40,688]: Building model: serving_sales_film\n",
      "[INFO][2022-07-24 18:04:40,717]: Successfully built model: serving_sales_film, rows inserted/updated: 958\n",
      "[INFO][2022-07-24 18:04:40,719]: Building model: serving_films_popular\n",
      "[INFO][2022-07-24 18:04:40,727]: Successfully built model: serving_films_popular, rows inserted/updated: 958\n"
     ]
    }
   ],
   "source": [
    "# execute each node in the dag in order using a for loop \n",
    "for node in dag: \n",
    "    build_model(model=node, engine=target_engine, models_path=\"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec7476cd5298a73f69e8eecc398cdeac6e308767034e2d84faebe029453106ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
